\documentclass[12pt]{article}

\usepackage[margin=0.75in,top=0.9in,bottom=1.5in]{geometry}
\usepackage{algorithm2e,amsmath,amssymb,amsthm,fancyhdr,tcolorbox,verbatim}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollary}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{exmp}{Example}[section]
\newtheorem{xca}[exmp]{Exercise}

\theoremstyle{remark}
\newtheorem*{rem}{Remark}
\newtheorem*{note}{Note}
\newtheorem{case}{Case}

%\fancypagestyle{plain}{
%\fancyhead{}
%\fancyhead[C]{\huge CMIMC 2017 Power Round}
%\fancyfoot{}
%\fancyfoot[C]{\thepage}
%}
%\pagestyle{plain}
\pagestyle{fancy}
\lhead{}
\chead{\includegraphics[scale=0.15]{CMIMC-header-2017.png}}
\rhead{}
\setlength{\headheight}{43pt}
\cfoot{Page \thepage}
\lfoot{}

\renewcommand{\Pr}{\textbf{Pr}}
\newcommand{\E}{\textbf{E}}
\newcommand{\pts}[1]{\lbrack\textbf{#1}\rbrack}

\begin{document}\thispagestyle{empty}

\begin{center}

\vspace*{70pt}

\includegraphics[scale=0.23]{cmimc-header-2017.png}

\includegraphics[scale=0.35]{Power-Header.png}

\end{center}

\vspace{40pt}

\begin{center}
\includegraphics[scale=0.20]{instruction-header.png}
\noindent\rule{17.7cm}{2pt}
\end{center}

\vspace{6pt}

\begin{enumerate}
\item Do not look at the test before the proctor starts the round.

\item This test consists of several problems, some of which are short-answer and some of which require proofs, to be solved within a time frame of \textbf{60 minutes}. There are \textbf{150 points} total.

\item Answers should be written and clearly labeled on sheets of blank paper. Each numbered problem should be \textit{on its own sheet}. If you have multiple pages, number them as well (e.g. 1/3, 2/3).

\item Write your team ID on the upper-right corner and the problem and page number of the problem whose solution you are writing on the upper-left corner on each page you submit. Papers missing these will not be graded. Problems with more than one submission will not be graded.

\item Write legibly. Illegible handwriting will not be graded.

\item In your solution for any given problem, you may assume the results of previous problems, even if you have not solved them. You may not do the same for later problems.

\item Problems are not ordered by difficulty. They are ordered by progression of content.

\item No computational aids other than pencil/pen are permitted.

\item If you believe that the test contains an error, submit your protest in writing to Doherty Hall 2302 prior to the end of lunch.
\end{enumerate}

\newpage

\section{Introduction}

Randomness is a phenomenon present in all areas of mathematics and science, for it allows us to model very complex systems in a way that is rather well-tamed by mathematicians. The CMIMC 2017 Power Round aims to explore how randomness can be used as a powerful tool in algorithms.

\section{Random Variables}

A \emph{random variable} is a variable that takes a given value with a certain probability. The set of all possible values a given random variable can take is called its \emph{sample space}; how often it takes a given value depends on its \emph{probability distribution}. We will only deal with discrete sample spaces and distributions. For instance, here are the two major distributions we will look at:

\begin{itemize}
\item \emph{Discrete uniform distribution}: We say $X\sim\text{Discrete Uniform}(N)$ if $X$ takes the values $\{1,2,\dots,N\}$ with equal probabilities.

\item \emph{Bernoulli distribution}: We say a random variable $X$ has the Bernoulli distribution -- denoted $X\sim\text{Bernoulli}(p)$ -- if $X$ takes the value $1$ with probability $p$ and $0$ with probability $1-p$.

\item \emph{Geometric distribution}: Let $X$ be the random variable corresponding to the number of samples from $\text{Bernoulli}(p)$ until we get a $1$. Then $X\sim\text{Geometric}(p)$.
\end{itemize}

A very important point to note is the difference between random variables and random numbers: random variables describe an entire distribution and have no numerical value, whereas random numbers are the numerical result of sampling from a distribution. For instance, the random variable corresponding to a coin toss describes all the possible outcomes and their respective probabilities; however, flipping the coin once and observing it to be heads corresponds to a random number.

\begin{defn}
We call two random variables $X$ and $Y$ \emph{independent} -- denoted $X\perp Y$ -- if \[\Pr[(X=a)\cap(Y=b)]=\Pr[X=a]\cdot\Pr[Y=b]\] for all $a,b$.
\end{defn}

We will now look at a very useful statistic in describing the distributions of random variables.

\begin{defn}
Suppose $X$ can take the values $X_1,X_2,\dots$. The \emph{expected value} of a random variable $X$, denoted by $\E(X)$, is defined to be \[\E[X]=X_1\cdot\Pr[X=X_1]+X_2\cdot\Pr[X=X_2]+\dots.\]
\end{defn}

\begin{enumerate}
\item \pts{6} Suppose $X\sim\text{Geometric}(p)$. Prove that $\E[X]=\frac1p$.

\begin{tcolorbox}
We have \[\E[X]=p\cdot1+(1-p)\cdot(1+\E[X])\implies\E[X]=\frac1p\]

\textbf{Alternate Solution.} The probability we see a $1$ for the first time on toss $i$ is $p\cdot(1-p)^{i-1}$. Thus, \[\E[X]=\sum_{i=1}^\infty i\cdot p\cdot(1-p)^{i-1}\] For any $0<\alpha<1$, we can compute $S:=\sum_{i=1}^\infty i\cdot\alpha^i$ since \begin{align*}\frac1\alpha\cdot S-S&=\sum_{i=1}^\infty i\cdot\alpha^{i-1}-\sum_{i=1}^\infty i\cdot\alpha^i\\&=\sum_{i=0}^\infty(i+1)\cdot\alpha^i-\sum_{i=1}^\infty i\cdot\alpha^i\\&=\sum_{i=0}^\infty\alpha^i=\frac1{1-\alpha}\implies S=\frac{\alpha}{(1-\alpha)^2}\end{align*} Finally, we obtain that \[\E[X]=\frac{p}{1-p}\cdot\frac{(1-p)}{(1-(1-p))^2}=\frac1p\]
\end{tcolorbox}

\end{enumerate}

Here are four very important properties of the expected value:

\begin{itemize}
\item $\E[c\cdot X]=c\cdot\E[X]$ and $\E[c+X]=c+\E[X]$, where $c$ is a constant.

\item $\E[X\cdot Y]=\E[X]\cdot\E[Y]$, if $X\perp Y$.

\item (Linearity of expectation) $\E[X+Y]=\E[X]+\E[Y]$.
\end{itemize}

\begin{enumerate}
\setcounter{enumi}{1}

\item \pts{8} Prove these four properties.

\begin{tcolorbox}
Let $p_i:=\Pr[X=X_i]$ and $q_j:=\Pr[Y=Y_j]$.

\begin{itemize}
\item $\E[c\cdot X]=\sum_i(c\cdot X_i)\cdot p_i=c\cdot\left(\sum_iX_i\cdot p_i\right)=c\cdot\E[X]$.

\item $\E[c+X]=\sum_i(c+X_i)\cdot p_i=c\cdot\sum_ip_i+\sum_iX_i\cdot p_i=c+\E[X]$.

\item $\E[X\cdot Y]=\sum_{i,j}(X_i\cdot Y_j)\cdot\Pr[X=X_i\cap Y=Y_j]=\sum_{i,j}X_i\cdot Y_j\cdot p_i\cdot q_j=(\sum_iX_i\cdot p_i)\cdot(\sum_jY_j\cdot q_j)=\E[X]\cdot\E[Y]$.

\item $\E[X+Y]=\sum_{i,j}(X_i+Y_j)\cdot\Pr[X=X_i\cap Y=Y_j]=\sum_i\sum_jX_i\cdot\Pr[X=X_i\cap Y=Y_j]+\sum_j\sum_iY_j\cdot\Pr[X=X_i\cap Y=Y_j]=\sum_iX_i\cdot p_i+\sum_jY_j\cdot q_j=\E[X]+\E[Y]$.
\end{itemize}
\end{tcolorbox}


\item \pts{4} Suppose $X\sim\text{Bernoulli}(0.75)$ and $Y\sim\text{Discrete Uniform}(6)$ are independent random variables. Define $X'$ and $Y'$ to be random variables satisfying $X'=6\cdot X+4$ and $Y'=3\cdot Y+2$. What is $\E[X'+Y']$ and $\E[X'\cdot Y']$?

\begin{tcolorbox}
We have $\E[X]=0\cdot0.25+1\cdot0.75=\frac34$ and $\E[Y]=1\cdot\frac16+2\cdot\frac16+\dots+6\cdot\frac16=\frac72$. Thus $\E[X']=\E[6\cdot X+4]=6\cdot\E[X]+4=\frac{17}2$ and $\E[Y']=\E[3\cdot Y+2]=3\cdot\E[Y]+2=\frac{25}2$. We have $\E[X'+Y']=\E[X']+\E[Y']=\frac{17}2+\frac{25}2=21$, and since $X'\perp Y'$, we know $\E[X'\cdot Y']=\E[X']\cdot\E[Y']=\frac{17}2\cdot\frac{25}2=\frac{425}4$.
\end{tcolorbox}


\item \pts{7} Suppose we randomly distribute $m\ge1$ balls into $n\ge1$ initially empty bins, so that each ball has an equal chance of being placed in each bin. What is the expected number of balls in the first bin? What is the expected number of empty bins?

\begin{tcolorbox}
For $1\le i\le m$, let $X_i$ be the random variable that is $1$ if ball $i$ lands in the first bin and $0$ otherwise. Note that $\E[X_i]=\Pr[\text{ball }i\text{ lands in the first bin}]=\frac1n$. Then we seek $\E[X_1+\dots+X_m]=\E[X_1]+\dots+\E[X_m]=\frac1n+\dots+\frac1n=\frac{m}n$.

For $1\le i\le n$, let $Y_i$ be the random variable that is $1$ if bin $i$ is empty and $0$ otherwise. Note that $\E[Y_i]=\Pr[\text{bin }i\text{ is empty}]=\left(1-\frac1n\right)^m$ Then we seek $\E[Y_1+\dots+Y_n]=\E[Y_1]+\dots+\E[Y_n]=\left(1-\frac1n\right)^m+\dots+\left(1-\frac1n\right)^m=n\cdot\left(1-\frac1n\right)^m$.
\end{tcolorbox}
\end{enumerate}

\section{Algorithms}

An \emph{algorithm} is a set of instructions for performing a task. For instance, the following two algorithms determine if a nonnegative integer is odd or even:

\begin{center}
\begin{minipage}[t]{8cm}
\vspace{0pt}
\begin{algorithm}[H]
\DontPrintSemicolon
\KwData{a nonnegative integer $n$}
\KwResult{whether $n$ is odd or even}
\While{$n\ge0$}{
\If{$n=0$}{
\Return{$n$ is even}\;
}
\ElseIf{$n=1$}{
\Return{$n$ is odd}\;
}
\Else{
$n\gets n-2$\;
}
}
\end{algorithm}
\end{minipage}
\begin{minipage}[t]{8cm}
\vspace{0pt}
\begin{algorithm}[H]
\DontPrintSemicolon
\KwData{a nonnegative integer $n$}
\KwResult{whether $n$ is odd or even}
\For{$i=0,2,4,6,\dots$}{
\If{$n=i$}{
\Return{$n$ is even}\;
}
\ElseIf{$n=i+1$}{
\Return{$n$ is odd}\;
}
}
\end{algorithm}
\end{minipage}
\end{center}

We will now detail a common convention in writing algorithms called \emph{pseudocode}. \noindent \textbf{It is not required that you follow these conventions, but you must be able to clearly describe the steps of your algorithms.} There are only a few conventions that we need to introduce:

\begin{center}
\begin{tabular}{l|l}
[variable name] $\gets$ [value] & assigns (or reassigns) [value] to [variable name] \\
\textbf{if} [condition] \textbf{then} & perform the indented instruction if [condition] \\
& is satisfied \\
\textbf{else if} [condition] \textbf{then} & perform the indented instruction if the previous \\ 
& \textbf{if} condition isn't satisfied but [condition] is satisfied \\
\textbf{else} & perform the indented instruction if none of the \\
& previous \textbf{if} conditions are satisfied \\
\textbf{while} [condition] \textbf{do} & iteratively perform the indented instruction as \\ & long as [condition] is satisfied \\
\textbf{for} [variable name] $=$ [sequence] \textbf{do} & iteratively perform the indented instruction for \\
& each value of [variable name] in the sequence \\
\textbf{return} [value] & the output of the algorithm
\end{tabular}
\end{center}

Algorithms generally require \emph{correctness} -- the algorithm always outputs the right answer -- and \emph{termination} -- the algorithm doesn't run infinitely.

\begin{enumerate}
\setcounter{enumi}{4}

\item \pts{4} Prove that the above two algorithms are correct and always terminate.
\end{enumerate}

\begin{tcolorbox}
For the first algorithm, we decrease $n$ at each step, so it must terminate since a nonnegative integer cannot decrease infinitely. Also, $n\pmod2$ is preserved at each step, so once $n\in\{0,1\}$ we know $n$ is even iff $n=0$ and $n$ is odd iff $n=1$.

For the second algorithm, $n-i$ decreases at each step, so it must terminate. Also, since $i$ is even, once $n\in\{i,i+1\}$ we will know $n$ is even iff $n=i$ and $n$ is odd iff $n=i+1$.
\end{tcolorbox}

We will also introduce a way to generate random numbers.

\begin{defn}
Let the function $\textbf{B}(p)$ return a random number $X\sim\text{Bernoulli}(p)$, and let the function $\textbf{D}(N)$ return a random number $X\sim\text{Discrete Uniform}(N)$.
\end{defn}

With these two basic functions, we can simulate many complex probabilistic events. For example, suppose we want to generate a random number $X$ according to the following distribution:

\begin{center}
\begin{tabular}{cccc}
\hline
value & $0$ & $1$ & $2$ \\
$\Pr[X=\text{value}]$ & $1/4$ & $1/2$ & $1/4$ \\
\hline
\end{tabular}
\end{center}

One algorithm that can simulate such a distribution is:

\begin{algorithm}
\DontPrintSemicolon
\KwData{none}
\KwResult{a random number with the above distribution}
$u\gets\textbf{B}(1/2)$\;
$v\gets\textbf{B}(1/2)$\;
\Return{$u+v$}\;
\end{algorithm}

\vspace{-5pt}

\begin{enumerate}
\setcounter{enumi}{5}

\item \pts{2} Prove that this algorithm accurately returns a random number with the above distribution.

\begin{tcolorbox}
We have four equally-likely outcomes on the value of $(u,v)$. If $(u,v)=(0,0)$, then $u+v=0$, which occurs with probability $1/4$. If $(u,v)=(0,1)$ or $(u,v)=(1,0)$, then $u+v=1$, which occurs with probability $1/4+1/4=1/2$. If $(u,v)=(1,1)$, then $u+v=2$, which occurs with probability $1/4$. This is the above distribution.
\end{tcolorbox}

\item \pts{10} Let $p$ be any real number in $[0,1]$. Using only calls to $\textbf{B}(1/2)$, devise an algorithm that terminates with probability $1$ and returns a random number $X\sim\text{Bernoulli}(p)$. Prove that your algorithm is correct and terminates with probability $1$. \textbf{Partial credit of at most 4 points will be awarded for solving the case of $p=1/3$.}

\begin{tcolorbox}
For $p=1/3$, we can use the following algorithm:

\begin{algorithm}[H]
\DontPrintSemicolon
\KwData{none}
\KwResult{a random number $X\sim\text{Bernoulli}(1/3)$}
\While{$0=0$}{
$u\gets\textbf{B}(1/2)$\;
$v\gets\textbf{B}(1/2)$\;
\If{$u+v=0$}{
\Return{$1$}\;
}
\ElseIf{$u+v=1$}{
\Return{$0$}\;
}
}
\end{algorithm}

By Problem 6, we know the distribution of the random variable $u+v$. The probability that this algorithm doesn't return on a given step is $1/4$, so the probability that it doesn't terminate after $n$ steps is $(1/4)^n\to0$ as $n\to\infty$. If it does terminate, then on the last step there is a $1:2$ probability of returning $1$ versus returning $0$, which is the desired distribution.
\end{tcolorbox}

\begin{tcolorbox}
For general $p$, let $p=0.b_1b_2b_3\dots$ be a binary representation of $p$. Our algorithm is:

\begin{algorithm}[H]
\DontPrintSemicolon
\KwData{none}
\KwResult{a random number $X\sim\text{Bernoulli}(p)$}
\For{$k=1,2,3,\dots$}{
$c_k\gets\textbf{B}(1/2)$\;
\If{$0.c_1c_2\dots c_k<0.b_1b_2\dots b_k$}{
\Return{$1$}\;
}
\If{$0.c_1c_2\dots c_k>0.b_1b_2\dots b_k$}{
\Return{$0$}\;
}
}
\end{algorithm}

The probability it doesn't terminate after $n$ steps is $(1/2)^n\to0$, the case where $(c_1,c_2,\dots,c_n)=(b_1,b_2,\dots,b_n)$. If it terminates in at most $n$ steps, then it returns $1$ for $b_1b_2\dots b_n$ values. There are $2^n$ possible outcomes in total, so the probability of this happening is \[\frac{b_1b_2\dots b_n}{2^n}\to p\text{ as }n\to\infty\] Now, the probability it returns $0$ is \[\frac{2^n-b_1b_2\dots b_n-1}{2^n}\to1-p\text{ as }n\to\infty\] as desired.
\end{tcolorbox}
\end{enumerate}

\newpage

And finally, we will discuss how to shuffle a deck of cards. Suppose we have $N\ge3$ cards $C_1,C_2,\dots,C_N$ lined up in a row, and we want to shuffle the deck so that it is equally likely to see any possible outcome. We have devised the following algorithm to perform this task:

\begin{algorithm}
\DontPrintSemicolon
\KwData{the cards $C_1,C_2,\dots,C_N$}
\KwResult{a uniformly random permutation of $C_1,C_2,\dots,C_N$}
\For{$i=1,2,\dots,N$}{
$j\gets\textbf{D}(N)$\;
swap the positions of $C_i$ and $C_j$\;
}
\end{algorithm}

\vspace{-10pt}

\begin{enumerate}
\setcounter{enumi}{7}

\item \pts{7} Prove that this algorithm is incorrect, i.e., it does not properly shuffle the cards.

\begin{tcolorbox}
Suppose the algorithm works.  There are $N^N$ possible outcomes of this algorithm, all equally likely, and there are $N!$ possible permutations, so $N!\mid N^N$.  Then \[N-1\mid N!\mid N^N,\] but $\gcd(N-1,N^N)=1$.  Thus $N-1\mid 1$, which is a contradiction.
\end{tcolorbox}

\item \pts{7} Fix the algorithm, and prove correctness.

\begin{tcolorbox}
Instead of choosing $j$ uniformly at random from $\{1,2,\dots,N\}$, choose it uniformly at random from $\{i,i+1,\dots,N\}$. That is, replace the line $j\gets\textbf{D}(N)$ with $j\gets i-1+\textbf{D}(N+1-i)$. This algorithm is correct since there are $N!$ possibilities, and we can get any given permutation $C_{\sigma(1)},C_{\sigma(2)},\dots,C_{\sigma(N)}$ if, at the $i$th iteration, we swap $C_{\sigma(i)}\in\{C_1,C_2,\dots,C_N\}\backslash\{C_{\sigma(1)},C_{\sigma(2)},\dots,C_{\sigma(i-1)}\}$ with the card at position $i$ for all $i=1,2,\dots,N$.
\end{tcolorbox}
\end{enumerate}

\section{Randomized Algorithms}

Finally we meet the significant subject: randomized algorithms. A \emph{randomized algorithm} is an algorithm that uses random variables. For instance, when the CMIMC staff plays chess, we determine who plays as white by flipping a coin, which is a simulation of $\textbf{B}(0.5)$. A \emph{deterministic algorithm} is an algorithm that always produces the same output when run on the same input. Note: this does not mean all randomized algorithms are nondeterministic.

Let's examine why randomized algorithms might be useful. We all know the game of battleship, where each player places his ships, and then each player tries to guess the coordinates of the ships. Well, the game of one-dimensional battleship involves the first player setting $k$ of the numbers $A[1],A[2],\dots,A[4k]$ equal to $1$ and the rest equal to $0$. The second player's goal is to guess some index $i$ for which $A[i]=1$, and after each guess the first player reveals if it is correct or not.

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
$0$ & $0$ & $1$ & $0$ & $0$ & $0$ & $1$ & $1$ & $0$ & $1$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ \\
\hline
\end{tabular}
\end{center}

You are the second player in a game of one-dimensional battleship.

\begin{enumerate}
\setcounter{enumi}{9}

\item \pts{2} Find, with proof, a deterministic algorithm for playing one-dimensional battleship that requires at most $3k+1$ guesses.

\begin{tcolorbox}
Iteratively guess $A[1],A[2],\dots$. If $A[1]=\dots=A[3k]=0$ fail then $A[3k+1]=1$.
\end{tcolorbox}

\item \pts{2} Suppose you play with a deterministic algorithm. Prove that there exists some configuration by the first player for which you require at least $3k+1$ guesses.

\begin{tcolorbox}
All deterministic algorithms can be characterized by a fixed sequence of guesses $\sigma:\mathbb{N}\to(1,2,\dots,4k)$, iteratively guessing $A[\sigma(1)],A[\sigma(2)],\dots$. Since there are $3k$ $0$s among $A[1],\dots,A[4k]$, there exists some configuration such that $A[\sigma(1)]=A[\sigma(2)]=\dots=A[\sigma(3k)]=0$.
\end{tcolorbox}

\item \pts{6} Find, with proof, a randomized algorithm for playing one-dimensional battleship such that the expected number of guesses your strategy requires is a constant independent of $k$. Your algorithm does not necessarily need to terminate.

\begin{tcolorbox}
On the $i$th guess, let $X_i=\textbf{D}(4k)$ and guess $A[X_i]$. Let $Y_i$ be the random variable that is $1$ if $A[X_i]=1$ and $0$ otherwise, and let $Z$ be the random variable corresponding to the number of guesses in this strategy. Since $Y_i\sim\text{Bernoulli}(0.25)$, we have $Z\sim\text{Geometric}(0.25)$, so $\E[Z]=4$ by Problem 1.
\end{tcolorbox}
\end{enumerate}

Sometimes, when we design an algorithm, instead of always outputting the right answer, we can design an algorithm that will output the wrong answer with a very small probability.

\begin{defn}
A \emph{Las Vegas algorithm} is an algorithm for which the expected run-time is small, but it always outputs the right answer. A \emph{Monte Carlo algorithm} is an algorithm for which the worst-case run-time is small, but it errs with a small probability.
\end{defn}

To demonstrate this idea, we will present a Monte Carlo algorithm to determine whether or not a number is prime. But first, we will state without proof that there is a function $\text{IsComposite}(n,a)$ that takes $n$ and a number $a\in\{2,3,\dots,n-1\}$ with the following properties:

\begin{itemize}
\item If $n$ is prime, then $\text{IsComposite}(n,a)$ always returns \textbf{false}.

\item If $n$ is composite, then $\text{IsComposite}(n,a)$ incorrectly returns \textbf{false} for less than half of the numbers $a\in\{2,3,\dots,n-1\}$.
\end{itemize}

Knowing this function, we can now describe this primality test algorithm:

\begin{algorithm}
\DontPrintSemicolon
\KwData{a positive integer $n$}
\KwResult{whether or not $n$ is prime}
\For{$i=1,2,\dots,100$}{
$a\gets1+\textbf{D}(n-2)$\;
\If{$\emph{IsComposite}(n,a)$}{
\Return{$n$ is composite}\;
}
}
\Return{$n$ is prime}
\end{algorithm}

\vspace{-10pt}

\begin{enumerate}
\setcounter{enumi}{12}

\item \pts{5} What is the probability this algorithm outputs ``$n$ is composite" when $n$ is actually prime? What is the probability that this algorithm outputs ``$n$ is prime" when $n$ is actually composite? How can we decrease these two probabilities?

\begin{tcolorbox}
If $n$ is prime, $\text{IsComposite}(n,a)$ always evaluates to \textbf{false}, so the algorithm never outputs ``$n$ is composite" and hence the probability is $0$.  If $n$ is composite, the algorithm outputs ``$n$ is prime" with probability less than $2^{-100}$. To decrease this probability, change $100$ to a larger number in the algorithm.
\end{tcolorbox}
\end{enumerate}

\section{Sorting}

The problem of sorting a list is a classical problem in any text on algorithms. Here we will present two sorting algorithms, one deterministic and one randomized, and compare their efficiencies. Sorting algorithms' efficiencies are generally measured by the number of comparisons they make, i.e., the number of times we check if $A[i]<A[j]$ for some $i,j$.

Consider the following deterministic sorting algorithm:

\newpage

\begin{algorithm}
\DontPrintSemicolon
\TitleOfAlgo{Sorting Algorithm \#1}
\KwData{a list $L=(L[1],L[2],\dots,L[n])$ of integers}
\KwResult{$L$ sorted in increasing order}
\For{$i=1,2,\dots,n$}{
\For{$j=i+1,\dots,n$}{
\If{$A[j]<A[i]$}{
swap $A[i]$ and $A[j]$\;
}
}
}
\end{algorithm}

\vspace{-10pt}

\begin{enumerate}
\setcounter{enumi}{13}

\item \pts{4} Prove that this algorithm is correct and terminates.

\begin{tcolorbox}
It clearly terminates since there are a finite number of steps. At the end of the inner \textbf{for} loop, $L[i]$ will have been replaced with $\min_{i\le j\le n}L[j]$. Thus, after each iteration of the outer \textbf{for} loop, we know that $(L[1],\dots,L[i])$ is a sorted list of the smallest elements of $L$.
\end{tcolorbox}

\item \pts{2} Find the number of comparisons this algorithm always makes.

\begin{tcolorbox}
It makes one comparison for each pair of $1\le i<j\le n$. That is $\binom{n}2=\frac{n(n-1)}2$ comparisons.
\end{tcolorbox}
\end{enumerate}

The second algorithm uses a technique called \emph{recursion}, which means it divides the problem into sub-problems of the same type as the original problem, solves the sub-problems, and combines the results. Remember, this is perfectly acceptable as long as we cover the base cases. So consider the following randomized sorting algorithm:

\newpage

\begin{algorithm}
\DontPrintSemicolon
\TitleOfAlgo{Sorting Algorithm \#2}
\KwData{a list $L=(L[1],L[2],\dots,L[n])$ of integers}
\KwResult{$L$ sorted in increasing order}
\If{$n=0,1$}{
\Return{$L$}\;
}
$p\gets\textbf{D}(n)$ ($L[p]$ is called a \emph{pivot} in this case)\;
$L_1,L_2\gets\emptyset$\;
\For{$i=1,2,\dots,p-1,p+1,\dots,n$}{
\If{$L[i]\le L[p]$}{
add $L[i]$ to $L_1$\;
}
\Else{
add $L[i]$ to $L_2$\;
}
}
use this algorithm to sort $L_1,L_2$\;
\Return{$[L_1,L[p],L_2]$}\;
\end{algorithm}

\begin{enumerate}
\setcounter{enumi}{15}

\item \pts{6} Prove that this algorithm is correct and terminates.

\begin{tcolorbox}
Each time the algorithm is called, it is called on lists of sizes strictly less than that of the previous call. Since $n=0,1$ are covered, it will eventually terminate.

To prove correctness, note that in the end $L_1$ is a sorted list of elements of $L$ that are $\le L[p]$ and $L_2$ is a sorted list of elements of $L$ that are $>L[p]$, so $[L_1,L[p],L_2]$ is $L$ sorted.
\end{tcolorbox}

\item \pts{8} Let $C$ be a variable denoting the number of comparisons this algorithm makes, and suppose the resulting sorted array is $\ell_1\le\ell_2\le\dots\le\ell_n$. Furthermore, let $A_{ij}$ denote the event that this algorithm at some point compares $\ell_i$ and $\ell_j$. Prove that \[\E[C]=\sum_{1\le i<j\le n}\Pr[A_{ij}].\]

\begin{tcolorbox}
Recall that $A_{ij}$ can happen at most once. Thus, let $B_{ij}$ be the random variable that is $1$ if $A_{ij}$ occurs and $0$ if $A_{ij}$ doesn't occur. By linearity of expectation, we have \[\E[C]=\E\left[\sum_{1\le i<j\le n}B_{ij}\right]=\sum_{1\le i<j\le n}\E[B_{ij}]=\sum_{1\le i<j\le n}\Pr[A_{ij}]\]
\end{tcolorbox}

\item \pts{6} Prove that $A_{ij}$ occurs if and only if the first pivot chosen from $\ell_i,\ell_{i+1},\dots,\ell_j$ is either $\ell_i$ or $\ell_j$.

\begin{tcolorbox}
If either $\ell_i$ or $\ell_j$ is chosen as a pivot, then it is compared to all other elements among $\ell_i,\ell_{i+1},\dots,\ell_j$, so $A_{ij}$ occurs. If $\ell_k$ is chosen as a pivot before $\ell_i$ or $\ell_j$, then they will be broken up into different sets and thus never compared.
\end{tcolorbox}

\item \pts{6} Prove that $\Pr[A_{ij}]=\frac2{j-i+1}$.

\begin{tcolorbox}
Since each of $\ell_i,\ell_{i+1},\dots,\ell_j$ have an equal probability of being chosen first as a pivot, the probability that $\ell_i$ or $\ell_j$ are chosen first is $\frac2{j-i+1}$. By Problem 18, this is $\Pr[A_{ij}]$.
\end{tcolorbox}

\item \pts{6} Let \[H_n:=\frac11+\frac12+\dots+\frac1n\] denote the $n^{\text{th}}$ harmonic number. Prove that $\E[C]=2(n+1)H_n-4n$.

\begin{tcolorbox}
By Problems 17 and 19, we have \begin{align*}\E[C]&=\sum_{i=1}^n\sum_{j=i+1}^n\frac2{j-i+1}=\sum_{i=1}^n\sum_{j=2}^{n-i+1}\frac2j\\&=\sum_{i=1}^n(2H_{n-i+1}-2)=2\sum_{i=1}^nH_i-2n\end{align*} Now we notice that \[[(i+1)H_i-i]-[iH_{i-1}-(i-1)]=i(H_i-H_{i-1})+H_i-1=H_i\] so that \[\sum_{i=1}^nH_i=(n+1)H_n-n\]
\end{tcolorbox}

\item \pts{4} In the worst case, how many comparisons are made?

\begin{tcolorbox}
Suppose every time we choose the smallest element in the set. Then the algorithm is identical to Sorting Algorithm \#1, so there are $\frac{n(n-1)}2$ comparisons made in the worst case.

\par More formally, let $W_n$ be the maximum number of comparisons on a list of size $n$.  We prove that $W_n=\tfrac{n(n-1)}2$ by strong induction on $n$.  Base cases are easy (in particular, one can check that this holds for $n=0$ and $n=1$).  For the inductive step, note that after the initial $n-1$ comparisons the list $L$ is split into two lists of size $k$ and $n-1-k$.  Thus \begin{align*}W_n &= n-1 + \max_{0\leq k\leq n-1}\left(W_k + W_{n-1-k}\right) = n-1 + \max_{0\leq k \leq n-1}\left(k^2-nk+\dfrac{n^2-3n+2}2\right) \\&= n-1 + \dfrac{n^2-3n+2}2 = \dfrac{n(n-1)}2.\end{align*}
\end{tcolorbox}

\item \pts{8} Now you will compare the efficiencies of Sorting Algorithm \#1 and Sorting Algorithm \#2. Let $a_n$ be the answer to Problem 15. Prove that \[\lim_{n\to\infty}\frac{2(n+1)H_n-4n}{a_n}=0.\]

\begin{tcolorbox}
Suppose $2^k\le n<2^{k+1}$. We have \begin{align*}H_n&=\frac11+\left(\frac12+\frac13\right)+\left(\frac14+\frac15+\frac16+\frac17\right)+\dots+\left(\frac1{2^k}+\dots+\frac1n\right)\\&\le\frac11+\left(\frac12+\frac12\right)+\left(\frac14+\frac14+\frac14+\frac14\right)+\dots+\left(\frac1{2^k}+\dots+\frac1{2^k}\right)\\&=1+1+1+\dots+\frac{n-2^k+1}{2^k}=k-1+\frac{n+1}{2^k}\le k+1<\log_2n+1\end{align*} Thus, \[\lim_{n\to\infty}\frac{2(n+1)(\log_2n+1)-4n}{n(n-1)/2}=0\] since the numerator grows with $n\log_2n$ and the denominator grows with $n^2$.
\end{tcolorbox}
\end{enumerate}

\section{Randomized Approximation Algorithms}

Often, it is unreasonable to find the exact solution to a problem because maybe finding the exact solution requires checking $2^{100}$ different cases. Therefore, sometimes it suffices to develop an \emph{approximation algorithm}, which is an algorithm that simply finds a very good solution to a problem. For example, we do not know any efficient algorithms (and none exist if ${\sf P}\neq{\sf NP}$!) to find an assignment of boolean variables that satisfy a given set of boolean formulas; however, there is an approximation an algorithm that can find an assignment satisfying an expected $87.5\%$ of the formulas.

We will investigate the following problem: there are $n$ people at a party, some of whom know each other. A subset of these people is called a \emph{cover} if, after we remove those people from the party, no two distinct people at the party know one another. The problem is, given a list of who knows who at the party, to find a cover of minimal size. The following approximation algorithm determines a cover that is close to minimal size:

\newpage

\begin{algorithm}
\DontPrintSemicolon
\KwData{a list $L$ of pairs $\{u,v\}$ of people who know each other}
\KwResult{a cover that is close to minimal size}
$S\gets\emptyset$\;
\For{$\{u,v\}\in L$}{
\If{$u,v\not\in S$}{
randomly choose $u$ or $v$ with equal probability\;
add the chosen vertex to $S$\;
}
}
\Return{$S$}\;
\end{algorithm}

\vspace{-10pt}

\begin{enumerate}
\setcounter{enumi}{22}

\item \pts{4} Prove that this algorithm indeed returns a cover.

\begin{tcolorbox}
Suppose $u$ knows $v$, but $u,v\not\in S$. Then this algorithm will have chosen at least one of $u,v$ will be chosen to be in $S$, contradiction. Thus $S$ is a cover.
\end{tcolorbox}

\item \pts{8} Let $C$ denote any cover of minimal size. Let $S_i$ denote the contents of $S$ after completing the $i$th iteration of the loop. Prove that, for all $i\ge0$, \[\E[|S_i\cap C|]\ge\E[|S_i\backslash C|].\]

\begin{tcolorbox}
We induct on $i$. For $i=0$, we have $0\ge0$. For general $i$, if one of $u,v$ is in $S$ then $S_{i+1}=S_i$ so we're done. Otherwise, we know at least one of $u,v$ belongs to $C$ as well. Thus, the left-hand side has probability at least $1/2$ of increasing by $1$, whereas the right-hand side has probability at most $1/2$ of increasing by $1$.
\end{tcolorbox}

\item \pts{6} Conclude that, after the algorithm terminates, \[\E[|S|]\le2\cdot|C|.\]

\begin{tcolorbox}
We have \[\E[|S|]=\E[|S\backslash C|]+\E[|S\cap C|]\le2\cdot\E[|S\cap C|]\le2\cdot|C|.\]
\end{tcolorbox}
\end{enumerate}

We have proven that this simple algorithm produces a cover that doesn't deviate too far from the minimal size of a cover.

Now consider the following variant of the original problem: for each person $v$ at the party, we assign a number $0<w_v\le1$ that describes how much we want him to stay at the party, where $1$ means we really want him to stay at the party, and $0.001$ means we really want to kick him out of the party. The number $w_v$ is called a \emph{weight}. The problem is to find a cover of minimal total weight. Notice that the original problem is the special case where all the weights are $1$.

\begin{enumerate}
\setcounter{enumi}{25}
\item \pts{8} For a set $T$, let \[W(T):=\sum_{v\in T}w_v\] We can modify the above algorithm by changing ``randomly choose $u$ or $v$ with equal probability" to ``randomly choose $u$ with probability $p_{uv}$ and $v$ with probability $1-p_{uv}$." Redefine $C$ to be any cover of minimal weight, instead of minimal size. Find, with proof, the value of $p_{uv}$ that ensures that for all $i\ge0$, \[\E[W(S_i\cap C)]\ge\E[W(S\backslash C)]\]

\begin{tcolorbox}
Let $p_{uv}:=\frac{w_v}{w_u+w_v}$. Then the left-hand side has probability at least $\frac{w_uw_v}{w_u+w_v}$ of increasing by $1$, whereas the right-hand side has probability at most $\frac{w_uw_v}{w_u+w_v}$ of increasing by $1$.
\end{tcolorbox}

\item \pts{4} Conclude that, after the algorithm terminates, \[\E[W(S)]\le2\cdot W(C).\]

\begin{tcolorbox}
We have \[\E[W(S)]=\E[W(S\backslash C)]+\E[W(S\cap C)]\le2\cdot\E[W(S\cap C)]\le2\cdot W(C)\]
\end{tcolorbox}
\end{enumerate}
\end{document}
